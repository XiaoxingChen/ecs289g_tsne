\documentclass{beamer}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{calc}
\usepackage{color}
\usepackage{commath}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage{mathtools}
\usepackage{siunitx}

\title{Visualizing Data using t-SNE}
\author{Taran Lynn, Xiaoli Yang, Xiaoxing Chen}

\begin{document}
\maketitle

% 
% 
% introduction to Isomap
% 
%

\begin{frame}
  \frametitle{Isometric Mapping (Isomap)}
  \framesubtitle{a non-linear dimensionality reduction method which tries to preserve the geodesic distances in the lower dimension}

  In geometry, a \textbf{geodesic} is commonly a curve representing in some sense the shortest path 
  between two points in a surface, or more generally in a Riemannian manifold.
  \begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{build/geodesic_distance.pdf}
  \end{figure}

\end{frame}

\begin{frame}
  \frametitle{Isometric Mapping (Isomap)}
  \framesubtitle{a non-linear dimensionality reduction method which tries to preserve the geodesic distances in the lower dimension}

  \begin{block}{1. Nearest neighbor search}
    Isomap starts by creating a neighborhood network.
  \end{block}

  \begin{block}{2. Shortest-path graph search}
    Isomap uses graph distance to the approximate geodesic distance between all pairs of points.
  \end{block}

  \begin{block}{3. Partial eigenvalue decomposition}
    And then, through eigenvalue decomposition of the geodesic distance matrix, it finds the low dimensional embedding of the dataset.
  \end{block}
  

\end{frame}


\begin{frame}
  \frametitle{Isometric Mapping (Isomap)}
  \begin{block}{Complexity}
    \[
      \underbrace{O[D \log(k) N \log(N)]}_{\text{nearest neighbors search}}  + 
      \underbrace{O[N^2(k + \log(N))]}_{\text{shortest-path graph search}} + 
      \underbrace{O[d N^2]}_{\text{partial eigenvalue decomposition}}
    \]
    \begin{itemize}
      \item $N$: number of training data points
      \item $D$: input dimension
      \item $k$: number of nearest neighbors
      \item $d$: output dimension
    \end{itemize}
  \end{block}
  
\end{frame}
% 
% 
% 
% introduction to LLE
% 
% 
% 
\begin{frame}
  \frametitle{Locally Linear Embedding (LLE)}
  \framesubtitle{A topology preserving manifold learning method}

  Assumptions:
  \begin{itemize}
    \item Data is well sampled i.e. density of the dataset is high.
    \item Dataset lies on a smooth manifold.
  \end{itemize}

  \begin{block}{1. Nearest neighbor search}
    A distance metric is needed to measure the distance between the two points and classify them as neighbors. 
    For example Euclidean, Mahalanobis, hamming and cosine. 
    Either e-neighborhood or K-nearest neighbors will be used to create a neighborhood matrix.
  \end{block}

  \begin{block}{2. Weight Matrix Construction}
    Each point of the dataset is reconstructed as a linear weighted sum of its neighbors.
  \end{block}

  \begin{block}{3. Partial Eigenvalue Decomposition}
    Create each point in lower dimension using its neighbors and local $W$ matrix. 
    The neighborhood graph and the local Weight matrix capture the topology of the manifold.
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Locally Linear Embedding (LLE)}
  \framesubtitle{A topology preserving manifold learning method}
  
  \begin{block}{Complexity}
    \[
      \underbrace{O[D \log(k) N \log(N)]}_{\text{nearest neighbors search}}  + 
      \underbrace{O[D N k^3]}_{\text{weight matrix construction}} + 
      \underbrace{O[d N^2]}_{\text{partial eigenvalue decomposition}}
    \]
    \begin{itemize}
      \item $N$: number of training data points
      \item $D$: input dimension
      \item $k$: number of nearest neighbors
      \item $d$: output dimension
    \end{itemize}
  \end{block}

  
  \begin{block}{Weakness: Sensitive to outliers and noise}
    Datasets have a varying density and it is not always possible to have a smooth manifold.
  \end{block}
  
\end{frame}

% 
% 
% 
% Sammon Mapping
% 
% 
% 
\begin{frame}
  \frametitle{Sammon Mapping }
  \framesubtitle{}

  Cost Function
  \[
    E_s = \frac{1}{\sum_{ij}   \| x_i - x_j \|}
    \sum_{i \neq j} 
    \frac{(\| x_i - x_j \| - \|y_i - y_j\|^2)^2}{\| x_i - x_j \|}
  \]

  Steps
  \begin{itemize}
    \item Distance calculation
    \item Distance matrix construction
    \item Minimizing the projection error
  \end{itemize}

  \begin{block}{Main Weakness}
    the importance of retaining small pairwise
  distances in the map is largely dependent on small differences in these pairwise distances. 
  In particular, a small error in the model of two high-dimensional points that are extremely close together
  results in a large contribution to the cost function.
  \end{block}

\end{frame}

% 
% 
% 
% t-SNE
% 
% 
%
\begin{frame}
  \frametitle{t-SNE}
  \framesubtitle{Keep the pairwise similarity in lower dimension}

  
  {Steps}
  \begin{itemize}
    \item Nearest neighbor search
    \item Pairwise similarity calculation
    \item Minimize dissimilarity cost function
  \end{itemize}

  {Complexity}
    \begin{itemize}
      \item Computational complexity: $O(N^2)$
      \item Memory complexity: $O(N^2)$
    \end{itemize}

    {Weakness}
    \begin{itemize}
      \item Dimensionality reduction for other purposes (reduce to dimension $d > 3$)
      \item Curse of intrinsic dimensionality 
      \item Non-convexity of the t-SNE cost function
    \end{itemize}    
    
\end{frame}

% 
% 
% 
% plot compare method
% 
% 
%
\begin{frame}
\frametitle{Algorithm Comparison}
\framesubtitle{Criteria: similarity preservation, overlapping, distortion, time}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{build/plot_compare_methods.pdf}
\end{figure}
\end{frame}

% 
% 
% 
% plot plot_handwrite_digits
% 
% 
%
\begin{frame}
  \frametitle{Algorithm Comparison}
  \framesubtitle{Criteria: similarity preservation, overlapping, distortion, time}
  \begin{figure}
    \centering
    \includegraphics[width=\linewidth]{build/plot_handwrite_digits.pdf}
  \end{figure}
  \end{frame}

% 
% 
% 
% Criteria
% 
% 
%
\begin{frame}
  \frametitle{Comparison and Conclusion}

  \begin{block}{1. Nearest neighbor search}
    All methods except Sammon mapping do NN Search. Sammon mapping scales the weight of closer neighbors by dividing a factor.
  \end{block}

  \begin{block}{2. Local structure preservation}
    \begin{itemize}
      \item Isomap: preserve geodesic distance (represented by graph distance)
      \item LLE: preserve manifold topology
      \item Sammon mapping: preserve weighted distances 
      \item t-SNE: maximize pairwise dissimilarity(represented by conditional probabilities)
    \end{itemize}
  \end{block}

  \begin{block}{3. Solving methods}
    Isomap and LLE use eigenvalue decomposition, Sammon mapping and t-SNE use gradient descent.
  \end{block}
    
\end{frame}

% 
% 
% 
% Conclusion
% 
% 
%
% \begin{frame}
%   \frametitle{Conclusion}
    
% \end{frame}
\end{document}
